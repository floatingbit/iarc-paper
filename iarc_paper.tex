\documentclass[12pt, letterpaper]{article}
\usepackage{iarc_latex_style}
\usepackage{amssymb,amsmath,listings,url,verbatim,graphicx}

\title{Beohawk: Autonomous Quadrotor}
\begin{document}
\maketitle
\begin{people}
\name{Rustom Jehangir}
\org{University of Southern California}
\name{Christopher Li}
\org{University of Southern California}
\end{people}

%1) Abstract	5
\begin{abstract}
	In this paper, we introduce a Micro UAV system that can explore an unknown indoor space without the assistance of a positioning system such as GPS. The robot takes in various kind of sensing measurements, handles them with probabilistic theories, and completes tasks such as stabilization and SLAM. \todo{Finish abstract at the end} (5 points)
\end{abstract}

%2) Introduction	5
%  a) Statement of the problem 
%  b) Conceptual solution to solve the problem
%    b1) Figure of overall system architecture 
%  c) Yearly Milestones
\section{Introduction}

\subsection{Problem Statement}

As the requirement of the current International Aerial Robotics Competition (IARC 2011), an aerial robot vehicle not heavier than 1.5 kilograms should explore an unknown, office-style floor of a building, localize itself according to the environment, identify and pick up a target object (a black USB drive specifically), and return to the starting point within given time limit. The robot should also complete the above tasks totally autonomously and reduce various types of random sensing and system noises. Additionally, the robot may also be intelligent to identify mission-related features (e.g. signature on each door of rooms) as well as observing and avoiding obstacles or surveillance devices while traversing the hallway.

\subsection{Conceptual Solution}

The Aerial Robotics Team (ART) of the USC Robotics Society (USCRS) purposes a quadrotor design of its aerial robot vehicle called \textit{Beohawk}, which consists of four motors located symmetrically at each end of a X-shape frame. The whole frame of the quadrotor is custom built by our team members to be within the required weight and size range. Various sensors, Inertial Sensor board, on-board computer and other facilities are located at the center of the frame. An off-board computing station is dedicated to the majority of computational tasks; it receives data from the on-board computer, fuse and process these data in order to make judgments on future movements of the robot, and then publish operating commands back. The communication is based on standard Wi-Fi network. Both computers have mission-specific programs written by our team. These programs run on the Robotic Operating System (ROS) and take advantages of many of the packages and tools made available by public contributors. By utilizing inertial and visual sensing data from various source and having these data handled by state-of-the-art algorithms, we hope our quadrotor to perform stabilization, navigation and mission planning successfully.

\subsubsection{Figure of Overall System Architecture}

Figure \eqref{fig:architecture} shows the basic system architecture of the quadrotor. The quadrotor's low level control including stability, attitude control, altitude control, and position control are all performed by the low-level control board. This board is an Arduino based board that has sensors and motor outputs. It also receives radio-control signals and allows control to be over-ridden by a human pilot.

\begin{figure}[h]
\centering
\includegraphics[width=14cm]{images/beohawk-system-arch.pdf}
\caption{General architecture of the Beohawk control system. \todo{This needs to be completed}.} 
\label{fig:architecture}
\end{figure}

\subsection{Yearly Milestones}

This is USC Robotics Sotiety's first appearance at IARC as well as the very first appearance in robotics competition. Our hardware team has developed three models for the frame in the past two years. Our latest model uses strong, lightweight carbon fiber to build up the frame. The software team has been researching on various kind of algorithms for UAV stabilization and navigation, and has tried to develop 3D visual navigation systems through both a probablistic approach and a graph approach,  which will be discussed later in detail. Our electronic team has designed customized circuit board that replace the commercialized alternatives (such as Arduino board). Regardless of the outcome of competition, the Aerial Robotics Team will continue refining hardware frames, circuit designs, as well as software algorithms and visualization interfaces.

% picuture of the frame (solidworks) 

%3) Air Vehicle	15
%  a) Propulsion and Lift System 
%  b) Guidance, Nav., and Control 
%    b1) Stability Augmentation System 
%    b2) Navigation 
%    b3) Figure of control system architecture 
%  c) Flight Termination System
\section{Air Vehicle}

\subsection{Propulsion and Lift System}
\emph{Beohawk} has four rotors, each an equal distance from the quadrotor's center.  Two opposite motors spin clockwise and the other two spin counter-clockwise, which generates a net torque of zero.  Hence, the quadrotor does not need a separate rotor to control yaw like a conventional helicopter, and can control yaw by adjusting the proportion of rotor speeds.  \todo{Todo: More details on these rotors}

\subsection{Guidance, Navigation, and Control}

\subsubsection{Stability Augmentation System}

Before a quadrotor flies anywhere, stablization should be done to keep it hovering successfully in the air instead of drifting or crashing unexpectedly from its original position. Simply having equal thrust in all four motors won't solve the problem because of the asymmetrical distribution of weight across the quadrotor's body. Instead, we apply a dynamic system for measuring the drift of the quadrotor's position as well as correcting this drift.

Several sensor measurements are proved to have strong, reliable relationships with the position (translation and/or rotation) of the robot. Monitoring their changes can give us very useful knowledge about how much the robot drifts from its original position. We use direct-cosine matrix algorithm (DCM) [citation needed] to provide an optimized result of the real-time orientation of the robot. This algorithm utilize the three-axis linear acceleration and angular velocity measurements from the onboard inertial measuring unit (also known as IMU). Since linear acceleration readings from accelerometer appear to have high accuracy in long term while angular velocity readings from gyroscope tend to drift through time, DCM algorithm actually uses acceleration information to correct the gyroscope readings, thereby giving an almost drift-free info about robot's orientation. In addition, we also use a downward-facing camera to calculate the optical flow [citation needed] of salient, time-independent visual features. By doing so, we are able to figure out the motion trajectory of this camera, a technique called “structure from motion” [citation needed]. A sonar sensor is also mounted on the bottom of the quadrotor. This devices measures a much more accurate relative height of the robot from the ground, and is used to improve the result of the optical flow. The cooperation of DCM algorithm for IMU and the structure-from-motion technique for visual camera ensures us to keep track of robot's linear and angular position in real time.

Finally, this system uses a PID control loop [citation needed] to handle the stabilization issue given these position-related measurements. This control loop constantly updates the proportion changes, integrals and derivatives of each monitoring variable, and to generate the motor commands that can best keep these figures away from drifting. The system also uses an Extended Kalman Filter to correct the position estimation from optical flow algorithm in order to get rid of unpredicted noises in visual feature recognition and matching.

\begin{figure}[h]
\centering
\includegraphics[width=12cm]{images/overview_tf.png}
\caption{Navigation system of Beohawk. \todo{This is a placeholder}.} 
\label{fig:navi}
\end{figure}

\subsubsection{Navigation}

Simultaneous localization and mapping (as known as SLAM for short) has been the central problem for  robotics application in unknown circumstances. The purpose of the SLAM is to predict, as accurate as possible, the position of the robot based on its knowledge of its surrounded environment. We obtain the 3D position of visual salient features, calculate the affine transformation of the camera through time. Combining this knowledge with the gyroscope and optical flow readings in the stabilization part, we can set up a optimization system to reduce the effect of random errors. Further planning of waypoints will be based on these results from the SLAM process.

In this project, we use a infrared-powered depth camera that can tell how far one given pixel is away from the camera. Combining this depth camera with a traditional RGB camera, we are able to create a 3D model (or “point cloud”) of the visual perception as well as matching them in 3D to figure out the camera trajectory. Blindly matching every point across point clouds requires a huge amount of time and energy to process; instead we only match the points from different clouds that we know are of the same thing in the real environment. Singular value decomposition is used to produce the translation and rotation matrix.

While we can get the 6 degree-of-freedom position and orientation status of the robot, we only need the Navigation done in 2D, because the place our robot will explore is a single plane floor. We plan to use the official 2D navigation package from ROS. Figure \eqref{fig:navi} shows the inputs to the navigation stack. Mission control is managed with a state machine that keeps track of the current objective.  \todo{Possibly a state diagram? To finish later}


\subsubsection{Figure of Control System Architecture}

\begin{figure}[h]
\centering
\includegraphics[width=14cm]{images/beohawk-system-arch.pdf}
\caption{Control system architecture. \todo{This needs to be completed}.} 
\label{fig:architecture}
\end{figure}

\todo{And an explanation}

\subsection{Flight Termination System}
Because the RC controller communicates directly with the Arduino, the safety pilot can terminate the operation of the quadrotor in the event it is needed.

%4) Payload	15
%  a) Sensor Suite 
%    a1) GNC Sensors 
%    a2) Mission Sensors 
%      a21) Target Identification 
%      a22) Threat Avoidance 
%  b) Communications 
%  c) Power Management System 
%  d) Sub-Vehicle (if any)
\section{Payload}
\subsection{Sensor Suite}
\subsubsection{Microsoft Kinect}
The Kinect uses an infrared laser array of 640x480 points to provide depth information at 2048 levels of sensitivity. Beohawk points the Kinect sensor down at a 45$^\circ$ angle.  A ROS package uses the Kinect to produce a point cloud, which gives Beohawk a plane of vision with depth information.  Within this point cloud, there should be a kink where the floor is.  This allows us to estimate altitude.  We detect walls in a similar fashion, so that we can avoid colliding with the wall.

\subsubsection{Camera}
Using a downward facing camera, the quadrotor runs an optical flow algorithm on the x86 processor to reduce drift.  A second, front-facing camera captures pictures for reading the signs above the doors.  The images are published through the wireless network to the base station, which runs SIFT to detect the Chief of Security's office via the sign above the door. The front camera is also responsible for detecting the blue security light.  The bottom camera is used to identify the flash drive.

\subsubsection{Sonar}
Though the quadrotor has an estimate of altitude from the Kinect, it may be noisy and as such, the quadrotor also employs sonar to determine altitude.  A \todo{model/part number} is installed facing downwards.

\subsubsection{Target Identification}

Since the USB is a black object located on a white table surface, simply using a edge or contrast detection algorithm will easily identify the USB as a salient feature. While the 3D visual sensor may not be enough accurate to tell the dimension of the USB, it can sense the much larger box containing the USB and therefore helps a lot in target identification. 

Identifying the signatures on the doors to different rooms is a tough task, because arabic alphabets can be hard to identify by using traditional OCR technology, especially in the case that our robot may look at the signature in one given perspective, which result in a affine transformation of the signature image that makes the identification process harder. We use a scale/rotation invarient feature detection algorithm called "SURF" to tackle this problem. Basically this algorithm tries to match the picture it sees and the template in the database by trying numerous cases of different transformations. This algorithm is proved successful in matching pictures regardless of scale, rotation or gradient constraints.

\subsubsection{Threat Avoidance}
\todo{How will we dodge the laser tripwires?}

\subsection{Communications}

A Wi-Fi network is established between an on-board computer and a ground station. This network is time-synchronized and supports functionalities on both machines to communicate with each other through the ROS message system. While camera sensors are well supported by ROS, inertial sensor data and motor command have to go through serial communication. Since serial communication has not been implemented in ROS distribution so far, we developed a serial communication node that provides a protocol for exchanging messages safely through serial communication, which greatly improves the reliability of whole system. The sensor board communicates with the power board with I$^2$C technology at \todo{n} Hz. It also connects to an RC control device that operates at \todo{n} GHz. Inertial sensors are read at \todo{n} Hz, while visual sensing streams has a rate of 30 Hz for 2D images and a rate of 15 Hz for depth images.

\subsection{Power Management System}
Beohawk is powered by a 7.4V 500mAh Lithium-Polymer battery pack, which provides enough energy to run at least ten minutes. A power regulator board controls the battery and motor speed.  In the event of low power, the base station initiates a controlled descent to avoid damage to the quadrotor. 

%5) Operations	10
%  a) Flight Preparations 
%    a1) Checklist(s) 
%  b) Man/Machine Interface
\section{Operations}

\subsection{Flight Preparations}
Before any flight is performed, batteries must be charged an an able human operator must be available to assume the control the RC controller.  For competition, the following checklist must be completed.

\subsubsection{Competition Checklist}
\begin{enumerate}
  \item Inspect and test hardware
  \item Launch ROS nodes
  \item Check software status
  \item Ensure RC link works
  \item Test hover in place
  \item Start mission control
\end{enumerate}

\subsection{Man-Machine Interface}
The base station displays Beohawk's current mission and status inferred from the sensor data returned by Beohawk.  If the link is terminated, Beohawk will  hover in place until the connection is restored or until the human operator assumes control. The human operator has an RC controller that communicates directly with the Arduino.


%6) Risk Reduction	15
%  a) Vehicle Status 
%    a1) Shock/Vibration Isolation 
%    a2) EMI/RFI Solutions 
%  b) Safety 
%  c) Modeling and Simulation 
%  d) Testing
\section{Risk Reduction}
\subsection{Vehicle Status}
Beohawk transmits battery information, sensor information, and current objective to the base station. 

\subsubsection{Shock/Vibration Isolation}
\todo{Keith has some stuff on this}

\subsubsection{EMI/RFI Solutions}
Because our sensors and electronics are not very sensitive to the interference from motors and wireless communications, we have not had to worry about EMI or RFI.

\subsection{Safety}

\subsection{Modeling and Simulation}
Hardware was modeled in SolidWorks.

\subsection{Testing}
We have done continuous testing on Beohawk as we developed it. \todo{hardware testing methodology}

For software, we did field tests to ensure the proper functioning of SLAM and vision algorithms.  SLAM was prototyped in MATLAB and re-written in C++ while the mission control was written in Python and has a unit test suite as well as a behavior test suite. 

\begin{table}[h]
\centering
\begin{tabular}{l  r  r  r}
                                       & Used  & Avail. & Perc. \\
  Number of Slices:                    &  684  & 4656  &  14\%  \\
  Number of Slice Flip Flops:          &  198  & 9312  &   2\%  \\
  Number of 4 input LUTs:              & 1316  & 9312  &  14\%  \\
  Number of IOs:                       &   37  &       &      \\
  Number of bonded IOBs:               &   36  &  232  &  15\%  \\
  Number of BRAMs:                     &    2  &   20  &  10\%  \\
  Number of MULT18X18SIOs:             &   10  &   20  &  50\%  \\
  Number of GCLKs:                     &    2  &   24  &   8\%  \\
\end{tabular}
\caption{Resource usage.}
\label{tab:usage}
\end{table}


%7) Conclusion	5 
\section{Conclusion (5)}
Use tables and figures to concisely state your point. A table title appears above the table it references and appears in all caps and centered, whereas a figure title appears beneath the figure and with only leading capitalization. Both table and figure titles are italicized as shown in the examples below:


%8) References	5
\bibliographystyle{IEEEbib}
\begin{thebibliography}{10}
\bibitem[1]{bib:kinectinfo} \url{http://git.marcansoft.com/?p=libfreenect.git;a=summary/}
\end{thebibliography}


\end{document}
